# EvaluaciÃ³n de CÃ³digo con Sandbox - DocumentaciÃ³n

## ğŸ“‹ DescripciÃ³n General

Se ha implementado un nuevo sistema de evaluaciÃ³n **hÃ­brido** para cÃ³digo que combina:

- **70% - Funcionalidad**: Basado en resultados reales de ejecuciÃ³n en sandbox
- **30% - Calidad**: EvaluaciÃ³n de IA sobre legibilidad, eficiencia y buenas prÃ¡cticas

## ğŸ¯ Ventajas del Sistema HÃ­brido

âœ… **MÃ¡s Objetivo**: La mayorÃ­a del puntaje (70%) proviene de tests reales, no de interpretaciÃ³n de IA
âœ… **MÃ¡s Justo**: Si el cÃ³digo pasa todos los tests, garantiza mÃ­nimo 70% de calificaciÃ³n
âœ… **Menos Costos**: Solo se usa IA para evaluar calidad (30%), reduciendo llamadas a OpenAI
âœ… **MÃ¡s RÃ¡pido**: EjecuciÃ³n de tests es instantÃ¡nea
âœ… **MÃ¡s Confiable**: No depende de interpretaciÃ³n subjetiva de IA

## ğŸ”§ ImplementaciÃ³n Backend

### Archivos Modificados

#### 1. `/assessments/views.py`

**Imports agregados:**

```python
from django.conf import settings
import json
import logging

logger = logging.getLogger(__name__)
```

**Nuevo mÃ©todo en `CandidateAnswerViewSet`:**

```python
@action(detail=True, methods=['post'])
def evaluate_code_sandbox(self, request, pk=None):
```

### 2. Endpoint Disponible

**URL:** `POST /api/assessments/answers/{id}/evaluate_code_sandbox/`

**AutenticaciÃ³n:** Requerida (JWT Token)

**Body esperado:**

```json
{
  "test_results": [
    {
      "test_case": "DescripciÃ³n del test",
      "input": "Datos de entrada",
      "expected_output": "Salida esperada",
      "actual_output": "Salida obtenida",
      "passed": true,
      "execution_time_ms": 1.23,
      "error": null
    }
  ],
  "total_tests": 3,
  "passed_tests": 2,
  "sandbox_success": true
}
```

**Respuesta:**

```json
{
    "id": 1,
    "question": {
        "id": 1,
        "question_text": "Implementa una funciÃ³n...",
        "programming_language": "python"
    },
    "candidate": 1,
    "code_answer": "def suma_pares(arr):...",
    "is_correct": true,
    "points_earned": 85,
    "feedback": "ğŸ”’ **EvaluaciÃ³n con Sandbox (ejecuciÃ³n real)**\nâœ… Tests pasados: 3/3\n\n...",
    "test_results": [...],
    "answered_at": "2025-12-17T10:30:00Z"
}
```

## ğŸ“Š LÃ³gica de CalificaciÃ³n

### 1. CÃ¡lculo de Funcionalidad (70%)

```python
functionality_score = (passed_tests / total_tests) * 70
```

### 2. EvaluaciÃ³n de Calidad con IA (30%)

La IA evalÃºa **SOLO** calidad, NO funcionalidad:

- **Legibilidad**: Â¿Es fÃ¡cil de entender?
- **Eficiencia**: Â¿Usa buen algoritmo?
- **Buenas PrÃ¡cticas**: Â¿Sigue convenciones?

Puntaje: 0-30 puntos

### 3. Score Final

```python
final_score = functionality_score + quality_score

# GarantÃ­a de mÃ­nimos
if is_correct and final_score < 70:
    final_score = 70  # MÃ­nimo si pasa todos los tests
```

## ğŸ”„ Flujo de Trabajo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend ejecuta cÃ³digo en Sandbox        â”‚
â”‚  (e0, Piston API, u otro servicio)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend recopila resultados de tests:    â”‚
â”‚  - Test 1: âœ… Pasado                       â”‚
â”‚  - Test 2: âœ… Pasado                       â”‚
â”‚  - Test 3: âŒ Fallido                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  POST /api/.../evaluate_code_sandbox/      â”‚
â”‚  Body: {                                    â”‚
â”‚    test_results: [...],                     â”‚
â”‚    total_tests: 3,                          â”‚
â”‚    passed_tests: 2,                         â”‚
â”‚    sandbox_success: true                    â”‚
â”‚  }                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend calcula:                           â”‚
â”‚  1. Funcionalidad = 2/3 * 70 = 46.67%      â”‚
â”‚  2. IA evalÃºa calidad = 22/30               â”‚
â”‚  3. Score final = 46.67 + 22 = 68.67%      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Backend actualiza CandidateAnswer:         â”‚
â”‚  - is_correct: false                        â”‚
â”‚  - points_earned: 68.67                     â”‚
â”‚  - feedback: "ğŸ”’ EvaluaciÃ³n con Sandbox..." â”‚
â”‚  - test_results: [...]                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ§ª Testing

### Prueba Manual con cURL

```bash
curl -X POST http://localhost:8000/api/assessments/answers/1/evaluate_code_sandbox/ \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -d '{
    "test_results": [
      {
        "test_case": "Suma de nÃºmeros pares",
        "input": "[1,2,3,4,5,6]",
        "expected_output": "12",
        "actual_output": "12",
        "passed": true,
        "execution_time_ms": 1.5,
        "error": null
      }
    ],
    "total_tests": 1,
    "passed_tests": 1,
    "sandbox_success": true
  }'
```

### Script de Prueba Python

Se incluye `test_sandbox_endpoint.py` para pruebas automatizadas.

```bash
# Editar el archivo y configurar ANSWER_ID y TOKEN
python test_sandbox_endpoint.py
```

## ğŸ“ Modelo de Datos

### Campo `test_cases` en Question

Ya existe en el modelo:

```python
test_cases = JSONField(default=list, blank=True,
                       help_text="Casos de prueba para validar cÃ³digo")
```

**Formato esperado:**

```python
[
    {
        "description": "Suma de nÃºmeros pares en [1,2,3,4,5,6]",
        "input": "[1,2,3,4,5,6]",
        "expected_output": "12"
    },
    {
        "description": "Array vacÃ­o",
        "input": "[]",
        "expected_output": "0"
    }
]
```

### Campo `test_results` en CandidateAnswer

Ya existe en el modelo:

```python
test_results = JSONField(default=dict, blank=True,
                         help_text="Resultados de test cases")
```

## âš™ï¸ ConfiguraciÃ³n Requerida

### 1. Variables de Entorno

En `.env`:

```env
OPENAI_API_KEY=sk-...
```

### 2. Dependencias

```bash
pip install openai
```

Ya instalado: `openai 2.8.0`

### 3. Migraciones

No se requieren nuevas migraciones. Los campos necesarios ya existen:

- `Question.test_cases` âœ…
- `CandidateAnswer.test_results` âœ…
- `CandidateAnswer.code_answer` âœ…

## ğŸ” Permisos

El endpoint `evaluate_code_sandbox` es accesible por:

- âœ… Usuarios autenticados (cualquier rol)
- âœ… El candidato dueÃ±o de la respuesta
- âœ… Administradores

## ğŸ“Š Feedback Generado

El sistema genera feedback estructurado con:

1. **Resultados de EjecuciÃ³n:**

   ```
   ğŸ”’ EvaluaciÃ³n con Sandbox (ejecuciÃ³n real)
   âœ… Tests pasados: 2/3

   âœ… Test 1: Suma de nÃºmeros pares
      Input: [1,2,3,4,5,6]
      Esperado: 12
      Obtenido: 12

   âŒ Test 2: Array vacÃ­o
      Input: []
      Esperado: 0
      Obtenido: null
      Error: TypeError: ...
   ```

2. **EvaluaciÃ³n de Calidad (IA):**

   ```
   ğŸ¤– EvaluaciÃ³n de Calidad (IA)
   El cÃ³digo muestra buena legibilidad con nombres descriptivos...
   ```

3. **Resumen:**
   ```
   ğŸ“Š Desglose de puntaje:
   - Funcionalidad (tests): 46.7/70
   - Calidad (cÃ³digo): 22.0/30
   - Total: 68.7/100
   ```

## ğŸ”„ Fallback a EvaluaciÃ³n Tradicional

Si `sandbox_success = false` o `total_tests = 0`:

```python
if not sandbox_success or total_tests == 0:
    return self.evaluate_code(request, pk)
```

El sistema automÃ¡ticamente usa el mÃ©todo anterior (`evaluate_code`) basado 100% en IA.

## ğŸš€ PrÃ³ximos Pasos

1. âœ… Endpoint implementado
2. âœ… Validaciones completadas
3. âœ… Migraciones aplicadas
4. â³ IntegraciÃ³n con frontend
5. â³ Pruebas end-to-end

## ğŸ“š Referencias

- Modelo OpenAI: `gpt-4o-mini`
- Temperature para calidad: `0.6` (balance entre creatividad y consistencia)
- Formato respuesta: `json_object` (garantiza JSON vÃ¡lido)

---

**Fecha de implementaciÃ³n:** 17 de diciembre de 2025  
**VersiÃ³n backend:** Django 5.2.7 + DRF 3.15.2  
**Estado:** âœ… Implementado y listo para integraciÃ³n

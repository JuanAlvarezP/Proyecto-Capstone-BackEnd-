"""
Servicio de integraci√≥n con OpenAI para generar pruebas t√©cnicas
"""
import json
import os
from openai import OpenAI
from django.conf import settings


class OpenAIAssessmentService:
    """Servicio para generar preguntas t√©cnicas usando OpenAI"""
    
    def __init__(self):
        api_key = getattr(settings, 'OPENAI_API_KEY', os.getenv('OPENAI_API_KEY'))
        if not api_key:
            raise ValueError("OPENAI_API_KEY no est√° configurada en settings o variables de entorno")
        self.client = OpenAI(api_key=api_key)
        
    def generate_quiz_questions(self, topic, difficulty="MEDIUM", num_questions=10, language="es"):
        """
        Genera preguntas de cuestionario t√©cnico
        
        Args:
            topic: Tema t√©cnico (ej: "Python avanzado", "React Hooks", "Algoritmos")
            difficulty: EASY, MEDIUM, HARD
            num_questions: Cantidad de preguntas a generar
            language: Idioma de las preguntas (es, en)
            
        Returns:
            Lista de diccionarios con preguntas
        """
        difficulty_map = {
            "EASY": "f√°cil, conceptos b√°sicos",
            "MEDIUM": "intermedio, aplicaci√≥n pr√°ctica",
            "HARD": "avanzado, casos complejos y optimizaci√≥n"
        }
        
        prompt = f"""Genera {num_questions} preguntas de opci√≥n m√∫ltiple sobre {topic} de nivel {difficulty_map.get(difficulty, 'intermedio')}.

IMPORTANTE: Responde √öNICAMENTE con un JSON v√°lido, sin texto adicional antes o despu√©s.

Formato JSON requerido:
{{
  "questions": [
    {{
      "question_text": "¬øPregunta aqu√≠?",
      "question_type": "MULTIPLE_CHOICE",
      "options": ["Opci√≥n A", "Opci√≥n B", "Opci√≥n C", "Opci√≥n D"],
      "correct_answer": "0",
      "explanation": "Explicaci√≥n detallada de por qu√© la respuesta es correcta",
      "points": 10
    }}
  ]
}}

Reglas:
- Cada pregunta debe tener exactamente 4 opciones
- correct_answer debe ser el √≠ndice (0-3) de la opci√≥n correcta
- Las preguntas deben ser t√©cnicas y relevantes para {topic}
- Incluye una explicaci√≥n clara de la respuesta correcta
- Var√≠a la dificultad dentro del nivel {difficulty_map.get(difficulty)}
- Idioma: {'espa√±ol' if language == 'es' else 'ingl√©s'}
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "Eres un experto en crear evaluaciones t√©cnicas de programaci√≥n. Respondes SOLO con JSON v√°lido."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return result.get("questions", [])
            
        except Exception as e:
            raise Exception(f"Error al generar preguntas con OpenAI: {str(e)}")
    
    def generate_coding_challenges(self, topic, difficulty="MEDIUM", num_challenges=3, language="python"):
        """
        Genera desaf√≠os de c√≥digo pr√°ctico
        
        Args:
            topic: Tema t√©cnico
            difficulty: EASY, MEDIUM, HARD
            num_challenges: Cantidad de desaf√≠os
            language: Lenguaje de programaci√≥n (python, javascript, java, etc.)
            
        Returns:
            Lista de diccionarios con desaf√≠os de c√≥digo
        """
        difficulty_map = {
            "EASY": "b√°sico, sintaxis fundamental",
            "MEDIUM": "intermedio, estructuras de datos y algoritmos",
            "HARD": "avanzado, optimizaci√≥n y patrones complejos"
        }
        
        prompt = f"""Genera {num_challenges} desaf√≠os de programaci√≥n en {language} sobre {topic} de nivel {difficulty_map.get(difficulty, 'intermedio')}.

IMPORTANTE: Responde √öNICAMENTE con un JSON v√°lido.

Formato JSON requerido:
{{
  "challenges": [
    {{
      "question_text": "Descripci√≥n del problema a resolver",
      "question_type": "CODE",
      "programming_language": "{language}",
      "code_snippet": "# Plantilla inicial del c√≥digo\\ndef solution():\\n    pass",
      "test_cases": [
        {{"input": "datos de entrada", "expected_output": "salida esperada", "description": "Caso 1"}},
        {{"input": "otros datos", "expected_output": "otra salida", "description": "Caso 2"}}
      ],
      "explanation": "Explicaci√≥n de la soluci√≥n √≥ptima",
      "points": 20
    }}
  ]
}}

Reglas:
- Incluye al menos 3 test cases por desaf√≠o
- El code_snippet debe tener una plantilla inicial √∫til
- Los test cases deben cubrir casos normales, edge cases
- Explicaci√≥n debe incluir la complejidad temporal y espacial
- Nivel: {difficulty_map.get(difficulty)}
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": f"Eres un experto en crear desaf√≠os de programaci√≥n en {language}. Respondes SOLO con JSON v√°lido."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.7,
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return result.get("challenges", [])
            
        except Exception as e:
            raise Exception(f"Error al generar desaf√≠os con OpenAI: {str(e)}")
    
    def evaluate_code_answer(self, question_text, candidate_code, test_cases, language="python", difficulty="MEDIUM"):
        """
        Eval√∫a una respuesta de c√≥digo usando OpenAI
        
        Args:
            question_text: Enunciado de la pregunta
            candidate_code: C√≥digo enviado por el candidato
            test_cases: Lista de casos de prueba
            language: Lenguaje de programaci√≥n
            difficulty: Nivel de dificultad (EASY, MEDIUM, HARD)
            
        Returns:
            Dict con evaluaci√≥n y feedback
        """
        
        # Mapeo de dificultad a criterios de evaluaci√≥n
        difficulty_criteria = {
            "EASY": {
                "funcionalidad": 70,
                "correctitud": 15,
                "legibilidad": 10,
                "eficiencia": 5,
                "min_score": 80,  # Si pasa todos los tests
                "description": "nivel B√ÅSICO/F√ÅCIL"
            },
            "MEDIUM": {
                "funcionalidad": 60,
                "correctitud": 20,
                "legibilidad": 10,
                "eficiencia": 10,
                "min_score": 75,  # Si pasa todos los tests
                "description": "nivel INTERMEDIO"
            },
            "HARD": {
                "funcionalidad": 50,
                "correctitud": 25,
                "legibilidad": 10,
                "eficiencia": 15,
                "min_score": 70,  # Si pasa todos los tests
                "description": "nivel AVANZADO"
            }
        }
        
        criteria = difficulty_criteria.get(difficulty, difficulty_criteria["MEDIUM"])
        
        prompt = f"""Eval√∫a el siguiente c√≥digo del candidato para un ejercicio de {criteria['description']}:

PREGUNTA:
{question_text}

C√ìDIGO DEL CANDIDATO ({language}):
```{language}
{candidate_code}
```

CASOS DE PRUEBA:
{json.dumps(test_cases, indent=2)}

üéØ ESCALA DE PUNTAJES QUE DEBES USAR:
- Si el c√≥digo funciona y pasa TODOS los tests ‚Üí M√çNIMO {criteria['min_score']}% (hasta 100%)
- Si el c√≥digo funciona y pasa la mayor√≠a de tests ‚Üí 60-{criteria['min_score']-1}%
- Si el c√≥digo funciona parcialmente ‚Üí 40-59%
- Si el c√≥digo tiene errores graves ‚Üí 0-39%

üìä CRITERIOS (usa estos pesos):
1. FUNCIONALIDAD ({criteria['funcionalidad']}%): ¬øFunciona? ¬øPasa los tests?
2. CORRECTITUD ({criteria['correctitud']}%): ¬øLa l√≥gica es correcta?
3. LEGIBILIDAD ({criteria['legibilidad']}%): ¬øEs claro?
4. EFICIENCIA ({criteria['eficiencia']}%): ¬øEs razonable?

‚ö†Ô∏è REGLAS OBLIGATORIAS:
‚úÖ Si "is_correct": true ‚Üí el "score_percentage" DEBE ser M√çNIMO {criteria['min_score']}%
‚úÖ Si TODOS los "test_results" tienen "passed": true ‚Üí M√çNIMO {criteria['min_score']}%
‚úÖ NO seas demasiado estricto con c√≥digo que funciona correctamente
‚úÖ Este es {criteria['description']}, ajusta expectativas seg√∫n el nivel

Responde SOLO con JSON:
{{
  "is_correct": true/false,
  "score_percentage": N√öMERO_ENTRE_0_Y_100,
  "feedback": "An√°lisis del c√≥digo destacando fortalezas primero",
  "strengths": ["fortaleza 1", "fortaleza 2"],
  "improvements": ["sugerencia 1", "sugerencia 2"],
  "test_results": [
    {{"test_case": 1, "passed": true/false, "message": "resultado del test 1"}},
    {{"test_case": 2, "passed": true/false, "message": "resultado del test 2"}}
  ]
}}

RECORDATORIO FINAL: Si marcas "is_correct": true, el score_percentage NO puede ser menor a {criteria['min_score']}."""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": f"Eres un evaluador de c√≥digo {language}. REGLA CR√çTICA: Si el c√≥digo funciona correctamente (is_correct=true), el score_percentage DEBE ser M√çNIMO {criteria['min_score']}%. Si todos los tests pasan, M√çNIMO {criteria['min_score']}%. Respondes SOLO JSON v√°lido. S√© JUSTO y GENEROSO con c√≥digo funcional."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,  # M√°s determin√≠stico para puntajes consistentes
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # VALIDACI√ìN ROBUSTA: Asegurar puntaje m√≠nimo
            score = result.get("score_percentage", 0)
            is_correct = result.get("is_correct", False)
            test_results = result.get("test_results", [])
            
            # Verificar si todos los tests pasaron
            all_tests_passed = False
            if test_results:
                all_tests_passed = all(t.get("passed", False) for t in test_results)
            
            # Si el c√≥digo es correcto O todos los tests pasaron, aplicar puntaje m√≠nimo
            if (is_correct or all_tests_passed) and score < criteria["min_score"]:
                result["score_percentage"] = criteria["min_score"]
                result["is_correct"] = True
                result["feedback"] = f"‚úÖ C√≥digo funcional que resuelve correctamente el problema. {result.get('feedback', '')}"
            
            if result.get("is_correct") and result.get("score_percentage", 0) < criteria["min_score"]:
                result["score_percentage"] = criteria["min_score"]
            
            return result
            
        except Exception as e:
            raise Exception(f"Error al evaluar c√≥digo con OpenAI: {str(e)}")